adding tokenization.

# Tokenization : is a way of separating a piece of text into smaller units called tokens. tokens can be word, sentence, char.
## 1.  Tokenizing words : split the paragraph into individual words.
#### EX : Given below example consist word_tokenize :
- we use the __word_tokenize__ method to split the paragraph into individual words.

![t](https://user-images.githubusercontent.com/29980448/96453698-7aa47980-1238-11eb-9553-247bd4abaf3c.jpg)



## 2.  Tokenizing Sentences : We can also tokenize the sentences in a paragraph by using  sent_tokenize.

#### EX : Given below example consist word_tokenize :
- we use the __sent_tokenize__ method to split the paragraph into sentenes.

![t](https://user-images.githubusercontent.com/29980448/96455112-63ff2200-123a-11eb-8015-d463242e92bb.png)

